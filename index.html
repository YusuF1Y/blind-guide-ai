
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Blind Guide AI</title>

  <!-- =======================
       ACCESSIBLE UI STYLING
       ======================= -->
  <style>
    body {
      background: black;
      color: white;
      font-family: Arial, sans-serif;
      text-align: center;
    }

    video {
      width: 90vw;
      max-width: 400px;
      margin-top: 20px;
      border-radius: 12px;
    }

    button {
      width: 600vw;
      max-width: 400px;
      height: 600px;
      font-size: 40px;
      font-weight: bold;
      margin-top: 20px;
      border-radius: 20px;
      border: none;
    }

    #micBtn {
      background: #00ff88;
      color: black;
    }

    #connectBtn {
      background: #0099ff;
      color: white;
    }

    button:active {
      opacity: 0.8;
    }
  </style>
</head>

<body>

  <h1>Blind Guide AI</h1>

  <!-- CAMERA VIEW -->
  <video id="video" autoplay playsinline></video>

  <!-- MAIN CONTROLS -->
  <button id="micBtn" onclick="startListening()">ðŸŽ¤ SPEAK</button>
  <button id="connectBtn" onclick="connectESP()">ðŸ”— CONNECT DEVICE</button>

<script>
/* =========================================================
   SECTION 1: CAMERA SETUP (USED FOR GEMINI VISION)
   ========================================================= */

const video = document.getElementById("video");

navigator.mediaDevices.getUserMedia({
  video: {
    facingMode: { ideal: "environment" }
  }
})

  .then(stream => video.srcObject = stream)
  .catch(err => console.error("Camera error:", err));

function captureImage() {
  const canvas = document.createElement("canvas");
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
  canvas.getContext("2d").drawImage(video, 0, 0);
  return canvas.toDataURL("image/jpeg");
}


/* =========================================================
   SECTION 2: TEXT TO SPEECH (AI SPEAKING BACK)
   ========================================================= */

/*function speak(text) {
  console.log("AI says:", text);
  const utter = new SpeechSynthesisUtterance(text);
  utter.rate = 1;
  utter.pitch = 1;
  speechSynthesis.cancel();
  speechSynthesis.speak(utter);
}*/

let selectedVoice = null;

function loadVoices() {
  const voices = speechSynthesis.getVoices();

  selectedVoice =
    voices.find(v => v.name.includes("Google") && v.lang === "en-US") ||
    voices.find(v => v.lang === "en-US");

  console.log("Using voice:", selectedVoice?.name);
}

speechSynthesis.onvoiceschanged = loadVoices;
loadVoices();

function speak(text) {
  speechSynthesis.cancel();

  const utter = new SpeechSynthesisUtterance(text);
  if (selectedVoice) utter.voice = selectedVoice;

  utter.rate = 1.35;   // FAST
  utter.pitch = 0.9;   // male natural
  utter.volume = 1;

  speechSynthesis.speak(utter);
}




/* =========================================================
   SECTION 3: SPEECH TO TEXT (MIC INPUT)
   ========================================================= */

function startListening() {
  if (!("webkitSpeechRecognition" in window)) {
    alert("Please use Chrome or Edge");
    return;
  }

  const recognition = new webkitSpeechRecognition();
  recognition.lang = "en-US";
  recognition.interimResults = false;
  recognition.maxAlternatives = 1;

  recognition.onstart = () => speak("Listening");

  recognition.onresult = async e => {
    const userText = e.results[0][0].transcript;
    console.log("User said:", userText);

    const image = captureImage();
    const reply = await askAI(userText, image);

    if (reply) {
      speak(reply);
      handleVibration(reply);   // ESP32 vibration mapping
    } else {
      speak("I could not understand the scene.");
    }
  };

  recognition.start();
}


/* =========================================================
   SECTION 4: GEMINI 2.5 FLASH (VISION + REASONING)
   ========================================================= */

async function askAI(text, image) {
  const API_KEY = "AIzaSyCazk4A28-fv4aBQLtyh5DfxW0ptAuwl3M";

  const res = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${API_KEY}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        contents: [{
          role: "user",
          parts: [
            {
              text: `
You are assisting a blind person.
Describe doors, exits, obstacles, but if they ask for anything else just answer in short.
Give short 1-2 sentences fast response, don't use asterisks and don't number steps, give clear directions using steps and left/right.
User said: ${text}`
            },
            {
              inlineData: {
                mimeType: "image/jpeg",
                data: image.split(",")[1]
              }
            }
          ]
        }]
      })
    }
  );

  const data = await res.json();
  console.log("Gemini raw response:", data);

  try {
    return data.candidates[0].content.parts
      .map(p => p.text)
      .join(" ");
  } catch {
    return null;
  }
}


/* =========================================================
   SECTION 5: ESP32 (ESP2) BLUETOOTH LOW ENERGY (BLE)
   ========================================================= */

let bleCharacteristic = null;

// UUIDs MUST MATCH ESP32 CODE
const SERVICE_UUID = "4419c79f-da6d-4391-bf2e-bc21a02445c7";
const CHAR_UUID    = "93c83e97-7c7e-45f0-8578-ee75c822f0d5";

async function connectESP() {
  try {
    const device = await navigator.bluetooth.requestDevice({
      filters: [{ name: "BlindGuide" }],
      optionalServices: [SERVICE_UUID]
    });

    const server = await device.gatt.connect();
    const service = await server.getPrimaryService(SERVICE_UUID);
    bleCharacteristic = await service.getCharacteristic(CHAR_UUID);

    speak("Device connected");
  } catch (e) {
    console.error("BLE error:", e);
    speak("Device connection failed");
  }
}

function sendBLE(command) {
  if (!bleCharacteristic) return;
  bleCharacteristic.writeValue(
    new TextEncoder().encode(command)
  );
}


/* =========================================================
   SECTION 6: AI RESPONSE â†’ VIBRATION COMMANDS
   ========================================================= */

function handleVibration(text) {
  text = text.toLowerCase();

  if (text.includes("left")) {
    sendBLE("L");
  } else if (text.includes("right")) {
    sendBLE("R");
  } else if (text.includes("stop")) {
    sendBLE("S");
  } else if (text.includes("obstacle") || text.includes("danger")) {
    sendBLE("D");
  } else {
    sendBLE("F"); // forward / clear
  }
}

</script>
</body>
</html>
